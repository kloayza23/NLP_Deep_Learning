{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la manipulacion automatica del lenguaje natural. El lenguage humano es ampliamente ambiguo, cambia y evoluciona.\n",
    "Tambien se puede decir que es una coleccion de terminos referentes a un procesamiento computacional automatico, esto incluye algoritmos que toma texto producido por el humano como entrada y algoritmos que producen texto parecido al natural como salida.\n",
    "El objetivo del lenguaje natural estadistico es realizar inferencia estadistica para los campos del lenguage natural. La inferencia estadistica se refiere en tomar alguna data y realizar alguna inferencia acerca de su distribucion.\n",
    "El uso de aprendizaje profundo or redes neuronales ayuda a desempeñar mejor inferencia en tareas especificas para el desarrollo de systemas terminales robustos.\n",
    "Linguistica es el estudio cientifico del lenguaje, incluyendo gramatica, semantica y fonetica.\n",
    "El objetivo de la ciencia linguistica es capaz de caracterizar y explicar la multitud de observaciones linguisticas que circulan alrededor de nosotros.\n",
    "La linguistica computacional es el moderno estudio de la linguistica usando herramientas informaticas.\n",
    "La linguistica computacional es lo que ahora se conoce como el procesamiento de lenguage natural.\n",
    "Los metodos de manejo de datos para el procesamiento de lenguaje natural han llegado a ser tan populares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Es un campo del machine learning, aqui presento una grafica del porque es aconsejable utilizar deep learning, donde deep se refiere al numero de capaz de la red, dentro de las caracteristicas de deep learning, se tiene:\n",
    "\n",
    "* Tiene la habilidad para desempeñar la extraccion automatica de caracteristicas de un conjuunto de datos(Esto tambien se conoce como feature learning).\n",
    "* Son algoritmos con la habilidad para descubrir y aprender buenas representaciones usando feature learning.\n",
    "* Deep learning como representacion jerarquica de aprendizaje es un enfoque escalable para construir sistemas de reconocimiento de objetos\n",
    "* Los metodos deep learning ofrecen la oportunidad de nuevos enfoques de modelo para el desafio de los problemas de lenguage natural como la prediccion de secuencia a secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning con Keras\n",
    "\n",
    "* Primero se define el numero de entradas esperadas, un ejemplo de uso es un modelo de perceptron multicapa, con 2 entradas en la capa visible, 5 entradas escondidas y una entrada en la capa de salida.\n",
    "* Por lo general Keras generalmente toma las capas separadas, y cumple el rol de transformar la data de entrada para un modelo predictivo.\n",
    "* La funcion de activacion es importante para la capa de salida, este definira el formato que le prediccion tomara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 551\n",
      "Trainable params: 551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "visible = Input(shape=(10,))\n",
    "hidden1 = Dense(10, activation='relu')(visible)\n",
    "hidden2 = Dense(20, activation='relu')(hidden1)\n",
    "hidden3 = Dense(10, activation='relu')(hidden2)\n",
    "output = Dense(1, activation='sigmoid')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 61, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 16)        8208      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 13, 13, 10)        170       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 13, 13, 1)         11        \n",
      "=================================================================\n",
      "Total params: 8,933\n",
      "Trainable params: 8,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "visible = Input(shape=(64,64,1))\n",
    "conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(16, kernel_size=4, activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "hidden1 = Dense(10, activation='relu')(pool2)\n",
    "output = Dense(1, activation='sigmoid')(hidden1)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "visible = Input(shape=(100,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir texto con espacios en blanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir texto, selecionando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir texto con espacios en blanco, removiendo puntuaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracteres a remover:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n",
      "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted']\n",
      "----------------------------------------------\n",
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(\"Caracteres a remover: \",string.punctuation)\n",
    "print(\"\")\n",
    "words = text.split()\n",
    "print(words[:30])\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "stripped = [re_punc.sub('', w) for w in words]\n",
    "print(\"----------------------------------------------\")\n",
    "print(stripped[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'whats', 'happened', 'to', 'me', 'he', 'thought', 'it', 'wasnt', 'a', 'dream', 'his', 'room', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# put all words with lowercases\n",
    "words = [word.lower() for word in stripped]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His room,\n",
      "a proper human room although a little too small, lay peacefully\n",
      "between its four familiar walls.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin']\n"
     ]
    }
   ],
   "source": [
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de pasos para limpiar el texto: \n",
    "\n",
    "* Load the raw text.\n",
    "* Split into tokens.\n",
    "* Convert to lowercase.\n",
    "* Remove punctuation from each token.\n",
    "* Filter out remaining tokens that are not alphabetic.\n",
    "* Filter out tokens that are stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-14-c58017aa5dde>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-c58017aa5dde>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    words = [w for w in words if not w in stop_words]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'He', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as', 'he', 'look', '.', '``', 'what', \"'s\", 'happen', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Text Cleaning Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El modelo Bag of Words(BOW)\n",
    "\n",
    "No se puede trabajar con texto directamente cuando se usa algoritmos de machine learning, es necesario convertir texto a numeros. Entre los algoritmos de machine learning se puede desempeñar clasificacion de documentos, donde cada documento es una entrada y una label clase es la salida, para los algoritmos predictivos. Estos algoritmos toman vectores numericos como entrada, por lo que es necesario convertir documentos de texto a vectores numericos de longitud fija.\n",
    "\n",
    "Un modelo efectivo para trabajar con documentos de texto en machine learning es el modelo Bag of words(BOW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count with CountVectorizer\n",
    "\n",
    "Esto permite tokenize una coleccion de documentos de texto y construir un vocabulario de palabras conocidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 10, 'dog': 1, 'fox': 2, 'over': 8, 'of': 7, 'true': 11, 'quick': 9, 'in': 3, 'lazy': 5, 'brown': 0, 'jumped': 4, 'moment': 6}\n",
      "(1, 12)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 1 1 1 3 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox Jumped over the lazy dog in the moment of true\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# here i am testing a new document in the vocabulary, previously built.\n",
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencias de palabras con TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'over': 5, 'fox': 2, 'dog': 1, 'quick': 6, 'lazy': 4, 'brown': 0, 'thedog': 8, 'jumped': 3}\n",
      "[ 1.69314718  1.69314718  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.28768207  1.69314718]\n",
      "(1, 9)\n",
      "[[ 0.33535157  0.33535157  0.25504351  0.33535157  0.33535157  0.33535157\n",
      "   0.33535157  0.51008702  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog\",\"Thedog.\",\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing con HashingVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Text Data with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder utilizar los modelos de deep learning o machine learning, es necesario trabajar con data numerica, por lo tanto para poder trabajar con texto, primero hay que hacer el proceso de encodificar la data de texto a numero. Se puede realizar este proceso utilizando keras, por medio del Tokenizer API, que puede ser ajustado para el entrenamiento de la data y ademas usado para encodificar los documentos de entrenamiento, validacion y pruebas.\n",
    "Hay 4 partes se realizar la encoficacion:\n",
    "* Dividir las palabras con text_to_word_sequence\n",
    "* Encoding con one_hot\n",
    "* Hash encoding con hashing_trick\n",
    "* Tokenizer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir palabras con text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding con one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[5, 9, 6, 2, 9, 9, 5, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash enconding con hashing_trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!','Good work','Great effort','nice work','Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "-----------------------------------------------------\n",
      "5\n",
      "-----------------------------------------------------\n",
      "{'work': 1, 'done': 3, 'nice': 7, 'good': 4, 'effort': 6, 'great': 5, 'well': 2, 'excellent': 8}\n",
      "-----------------------------------------------------\n",
      "{'work': 2, 'done': 1, 'nice': 1, 'good': 1, 'effort': 1, 'great': 1, 'well': 1, 'excellent': 1}\n"
     ]
    }
   ],
   "source": [
    "# word counts: Un diccionario de palabras con su respectivo contador\n",
    "print(t.word_counts)\n",
    "print(\"-----------------------------------------------------\")\n",
    "# word docs: Un contador entero del total de numero de documentos Tokenizer.\n",
    "print(t.document_count)\n",
    "# word index: Un diccionario de palabras y sus numero asignador unicos\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(t.word_index)\n",
    "# document count: Un diccionario de palabras y cuantos documentos hay en tal diccionario\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La funcion texts to matrix() en el Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "-----------------------------------------------------\n",
      "5\n",
      "-----------------------------------------------------\n",
      "{'work': 1, 'done': 3, 'nice': 7, 'good': 4, 'effort': 6, 'great': 5, 'well': 2, 'excellent': 8}\n",
      "-----------------------------------------------------\n",
      "{'work': 2, 'done': 1, 'nice': 1, 'good': 1, 'effort': 1, 'great': 1, 'well': 1, 'excellent': 1}\n",
      "-----------------------------------------------------\n",
      "[[ 0.  0.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(t.document_count)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(t.word_index)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(t.word_docs)\n",
    "print(\"-----------------------------------------------------\")\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de bag of word es una forma de representar los datos de texto cuando se requiere modelar texto con machine learning.\n",
    "El modelo de bag of word tiene un gran exitos en los problemas de modelamiento de lenguage y clasificacion de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El problema con el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un metodo para extraer caracteristicas del texto es el modelo bag of word. Se llama bag of word, debido a que no es importante el orden o la estructura de como estan las palabras, el modelo solo se encarga de saber si ocurren o no palabras conocidas en el documento. Entonces la vision es saber si 2 documentos son similares, y ellos lo son si tienen contenido similar. La complejidad del modelo se da en como se diseña el vocabulario de las palabras conocidas(tokens), y cual es la calificacion de cada palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus of document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 4 documents\n",
    "docs = ['It was the best of times,',\n",
    "'it was the worst of times,',\n",
    "'it was the age of wisdom,',\n",
    "'It was the age of foolishness,']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recolectar el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it was the best of times\n",
      "it was the worst of times\n",
      "it was the age of wisdom\n",
      "it was the age of foolishness\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "stripped = [re_punc.sub('', w) for w in docs]\n",
    "words = [word.lower() for word in stripped]\n",
    "[print(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseñar el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 3, 'the': 5, 'foolishness': 2, 'worst': 9, 'best': 1, 'of': 4, 'wisdom': 8, 'was': 7, 'times': 6, 'age': 0}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(words)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear Vectores de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  1.  1.  1.  0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  1.  1.  1.  1.  0.  1.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(words)\n",
    "encoded_docs = t.texts_to_matrix(words, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('it', 4), ('was', 4), ('the', 4), ('best', 1), ('of', 4), ('times', 2), ('worst', 1), ('age', 2), ('wisdom', 1), ('foolishness', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(t.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparacion de Data para un analisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here It load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory):        \n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'txt_sentoken/neg'\n",
    "process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a utilizar el modelo Bag of Word, para limpiar la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir el texto en tokens\n",
    "\n",
    "Voy a correr un documento, para comprobar con que data estoy tratando, y ver que tengo que limpiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the document\n",
    "filename = 'txt_sentoken/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo al resultado, podria hacer lo siguiente:\n",
    "* Remover la puntuaciones de las palabras\n",
    "* Remover tokens que son solo puntuaciones\n",
    "* Remover tokens que contienen numeros\n",
    "* Remover tokens que contienen solo un caracter\n",
    "* Remover tokens que no tienen mucho sentido.\n",
    "\n",
    "Para realizar estas operaciones, se me ocurrieron algunas ideas:\n",
    "\n",
    "* Filtrar puntuaciones usando expresiones regulares\n",
    "* Remover tokens de solo puntuacion o numeros usando la funcion isalpha()\n",
    "* remover stopwords usando nltk\n",
    "* filtrar tokens cortos revisando la longitud del token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46557\n",
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep tokens with > 5 occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14803\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with > 5 occurrence\n",
    "min_occurane = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save List(Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "    # load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcionalidad principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# prepare negative reviews\n",
    "negative_lines = process_docs('txt_sentoken/neg', vocab)\n",
    "save_list(negative_lines, 'negative.txt')\n",
    "# prepare positive reviews\n",
    "positive_lines = process_docs('txt_sentoken/pos', vocab)\n",
    "save_list(positive_lines, 'positive.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "    # load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remover palabras con poca frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25767\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codigo principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "25767\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion para preparar un documento\n",
    "\n",
    "Carga el documento, lo limpia, filtra los tokens que no estan en el vocabulario y retorna un documento como un string con espacios en blanco separado por tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de documentos en linea\n",
    "\n",
    "Espera un directorio, un vocabulario y retorna una lista de documentos procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set con la salida labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar vocabulario y las variables X y Y del predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 1800\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all training reviews\n",
    "docs, labels = load_clean_dataset(vocab)\n",
    "# summarize what we have\n",
    "print(len(docs), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir el data set a vectores de Bag of Words(BOW)\n",
    "\n",
    "El Api de keras sera usado para convertir el data set a vectores de documentos codificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 25768) (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de analisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer modelo de analisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuracion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capa de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.1556 - acc: 0.9939\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.1429 - acc: 0.9961\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.1314 - acc: 0.9961\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.1209 - acc: 0.9961\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.1113 - acc: 0.9972\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.1028 - acc: 0.9978\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0949 - acc: 0.9983\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0876 - acc: 0.9989\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0812 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0752 - acc: 1.0000\n",
      "Test Accuracy: 90.500000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# fit network\n",
    "model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, np.array(ytest), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparando metodos de calificacion de palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare bag-of-words encoding of docs\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode training data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a neural network model\n",
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 30\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        model = Sequential()\n",
    "        model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # compile network\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        # fit network\n",
    "        model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)\n",
    "        # evaluate\n",
    "        loss, acc = model.evaluate(Xtest, np.array(ytest), verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.5329 - acc: 0.7339\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0953 - acc: 0.9856\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0214 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 7.6027e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 5.3517e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 3.9581e-04 - acc: 1.0000\n",
      "1 accuracy: 0.93\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.4611 - acc: 0.8033\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0537 - acc: 0.9972\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.7629e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.2403e-04 - acc: 1.0000\n",
      "2 accuracy: 0.935\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.4778 - acc: 0.7706\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0642 - acc: 0.9944\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0167 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.4933e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.5996e-04 - acc: 1.0000\n",
      "3 accuracy: 0.93\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.4681 - acc: 0.7767\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0600 - acc: 0.9944\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0166 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 7.9644e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 5.8124e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.4399e-04 - acc: 1.0000\n",
      "4 accuracy: 0.935\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.4799 - acc: 0.7800\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0776 - acc: 0.9856\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.1879e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.7725e-04 - acc: 1.0000\n",
      "5 accuracy: 0.925\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4726 - acc: 0.8028\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0569 - acc: 0.9961\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.8166e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.1265e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.3520e-04 - acc: 1.0000\n",
      "6 accuracy: 0.93\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4668 - acc: 0.7878\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0582 - acc: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.7632e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.7539e-04 - acc: 1.0000\n",
      "7 accuracy: 0.935\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4764 - acc: 0.7683\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0556 - acc: 0.9950\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0163 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.2914e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.2141e-04 - acc: 1.0000\n",
      "8 accuracy: 0.92\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4755 - acc: 0.7744\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0618 - acc: 0.9933\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.3368e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 5.9512e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.4449e-04 - acc: 1.0000\n",
      "9 accuracy: 0.94\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4773 - acc: 0.7756\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0611 - acc: 0.9944\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0171 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.4598e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.9136e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.2483e-04 - acc: 1.0000\n",
      "10 accuracy: 0.925\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4827 - acc: 0.7711\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0606 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0166 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.1520e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.7521e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.1955e-04 - acc: 1.0000\n",
      "11 accuracy: 0.93\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4610 - acc: 0.7983\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0542 - acc: 0.9967\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.1468e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.2906e-04 - acc: 1.0000\n",
      "12 accuracy: 0.915\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4714 - acc: 0.7800\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0568 - acc: 0.9928\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.4543e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.6630e-04 - acc: 1.0000\n",
      "13 accuracy: 0.92\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4804 - acc: 0.7694\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0639 - acc: 0.9950\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.3990e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.0790e-04 - acc: 1.0000\n",
      "14 accuracy: 0.925\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4698 - acc: 0.7817\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0557 - acc: 0.9967\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.0699e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.0398e-04 - acc: 1.0000\n",
      "15 accuracy: 0.92\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.5122 - acc: 0.7606\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0843 - acc: 0.9911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.4321e-04 - acc: 1.0000\n",
      "16 accuracy: 0.945\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4904 - acc: 0.7694\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0740 - acc: 0.9911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.6399e-04 - acc: 1.0000\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 6.9572e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.2445e-04 - acc: 1.0000\n",
      "17 accuracy: 0.91\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4670 - acc: 0.7789\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0557 - acc: 0.9967\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.7156e-04 - acc: 1.0000\n",
      "18 accuracy: 0.935\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4721 - acc: 0.7867\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0570 - acc: 0.9956\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.8332e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.9095e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.0197e-04 - acc: 1.0000\n",
      "19 accuracy: 0.93\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4658 - acc: 0.7972\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0558 - acc: 0.9956\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.6838e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.3526e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.7943e-04 - acc: 1.0000\n",
      "20 accuracy: 0.92\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4688 - acc: 0.7822\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0598 - acc: 0.9944\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.3055e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.6029e-04 - acc: 1.0000\n",
      "21 accuracy: 0.925\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4601 - acc: 0.7933\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0474 - acc: 0.9956\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 8.7192e-04 - acc: 1.0000\n",
      "22 accuracy: 0.94\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4586 - acc: 0.7828\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0468 - acc: 0.9950\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "23 accuracy: 0.915\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4604 - acc: 0.7806\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0512 - acc: 0.9928\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.8633e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.5659e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.0483e-04 - acc: 1.0000\n",
      "24 accuracy: 0.93\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4628 - acc: 0.7839\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0515 - acc: 0.9956\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.0977e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.0820e-04 - acc: 1.0000\n",
      "25 accuracy: 0.925\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4850 - acc: 0.7878\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0717 - acc: 0.9911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.6820e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.8641e-04 - acc: 1.0000\n",
      "26 accuracy: 0.91\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4656 - acc: 0.7794\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0591 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.7835e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.0129e-04 - acc: 1.0000\n",
      "27 accuracy: 0.935\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4719 - acc: 0.7867\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0571 - acc: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 8.6590e-04 - acc: 1.0000\n",
      "28 accuracy: 0.925\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4740 - acc: 0.7850\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0606 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 8.6009e-04 - acc: 1.0000\n",
      "29 accuracy: 0.925\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4937 - acc: 0.7667\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0719 - acc: 0.9889\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0178 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.6984e-04 - acc: 1.0000\n",
      "30 accuracy: 0.925\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4515 - acc: 0.8000\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0559 - acc: 0.9933\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.2623e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.1198e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.6748e-04 - acc: 1.0000\n",
      "1 accuracy: 0.895\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4992 - acc: 0.7439\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0819 - acc: 0.9839\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.0999e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.1651e-04 - acc: 1.0000\n",
      "2 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4844 - acc: 0.7628\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0763 - acc: 0.9872\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.5941e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.2584e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.7478e-04 - acc: 1.0000\n",
      "3 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4662 - acc: 0.7767\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0628 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      " - 2s - loss: 8.3805e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 5.8355e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.3253e-04 - acc: 1.0000\n",
      "4 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4530 - acc: 0.7856\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0548 - acc: 0.9928\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.7592e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.1044e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 3s - loss: 5.4134e-04 - acc: 1.0000\n",
      "5 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4569 - acc: 0.7817\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0522 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.6546e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.1383e-04 - acc: 1.0000\n",
      "6 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4658 - acc: 0.7778\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0573 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0147 - acc: 0.9994\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.0639e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.1991e-04 - acc: 1.0000\n",
      "7 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4487 - acc: 0.7956\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0483 - acc: 0.9928\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.7938e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.8494e-04 - acc: 1.0000\n",
      "8 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4618 - acc: 0.7828\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0544 - acc: 0.9928\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.4462e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.0081e-04 - acc: 1.0000\n",
      "9 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4523 - acc: 0.7806\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0469 - acc: 0.9933\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.2158e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.0159e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.5469e-04 - acc: 1.0000\n",
      "10 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4636 - acc: 0.7822\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0599 - acc: 0.9894\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.3907e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.5473e-04 - acc: 1.0000\n",
      "11 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.5082 - acc: 0.7372\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0834 - acc: 0.9839\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 8.2372e-04 - acc: 1.0000\n",
      "12 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4638 - acc: 0.7778\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0597 - acc: 0.9911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.6198e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.7691e-04 - acc: 1.0000\n",
      "13 accuracy: 0.91\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4708 - acc: 0.7772\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0528 - acc: 0.9894\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 8.1968e-04 - acc: 1.0000\n",
      "14 accuracy: 0.895\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4670 - acc: 0.7772\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0648 - acc: 0.9894\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.7959e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.8655e-04 - acc: 1.0000\n",
      "15 accuracy: 0.895\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4556 - acc: 0.7911\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0584 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0143 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.8326e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.2937e-04 - acc: 1.0000\n",
      "16 accuracy: 0.91\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4487 - acc: 0.7822\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0516 - acc: 0.9911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0125 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 8.9198e-04 - acc: 1.0000\n",
      "17 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4726 - acc: 0.7806\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0592 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.9148e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.9242e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.0604e-04 - acc: 1.0000\n",
      "18 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4652 - acc: 0.7817\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0550 - acc: 0.9917\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.3207e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.6738e-04 - acc: 1.0000\n",
      "19 accuracy: 0.91\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4487 - acc: 0.7939\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0418 - acc: 0.9956\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0112 - acc: 0.9994\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.1868e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 5.8975e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.4626e-04 - acc: 1.0000\n",
      "20 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4698 - acc: 0.7906\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0591 - acc: 0.9928\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0036 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.7821e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.1354e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.4377e-04 - acc: 1.0000\n",
      "21 accuracy: 0.895\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4659 - acc: 0.7694\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0639 - acc: 0.9883\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 8.6290e-04 - acc: 1.0000\n",
      "22 accuracy: 0.91\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4818 - acc: 0.7639\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0634 - acc: 0.9906\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.2942e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.3651e-04 - acc: 1.0000\n",
      "23 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4623 - acc: 0.7844\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0514 - acc: 0.9900\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.4819e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.6158e-04 - acc: 1.0000\n",
      "24 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4622 - acc: 0.7683\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0584 - acc: 0.9911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 9.8807e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 6.9720e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 5.1696e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 3.9690e-04 - acc: 1.0000\n",
      "25 accuracy: 0.905\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4585 - acc: 0.7744\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0533 - acc: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 3s - loss: 8.0480e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 3s - loss: 6.1672e-04 - acc: 1.0000\n",
      "26 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4576 - acc: 0.7806\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0499 - acc: 0.9939\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.0132 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.4636e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.4463e-04 - acc: 1.0000\n",
      "27 accuracy: 0.895\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4642 - acc: 0.7794\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0643 - acc: 0.9911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.0618e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.1304e-04 - acc: 1.0000\n",
      "28 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4579 - acc: 0.7833\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0583 - acc: 0.9889\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.0222e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.1774e-04 - acc: 1.0000\n",
      "29 accuracy: 0.895\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4636 - acc: 0.7817\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0505 - acc: 0.9922\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0143 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.7045e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.7589e-04 - acc: 1.0000\n",
      "30 accuracy: 0.895\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4712 - acc: 0.7728\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.3978e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.2985e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.8862e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.0079e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.3515e-04 - acc: 1.0000\n",
      "1 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4687 - acc: 0.7744\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0162 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.8657e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.6414e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.2395e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.3251e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.6542e-04 - acc: 1.0000\n",
      "2 accuracy: 0.89\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4838 - acc: 0.7611\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.6473e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.2858e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.7607e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.6792e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.9484e-04 - acc: 1.0000\n",
      "3 accuracy: 0.895\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4833 - acc: 0.7644\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0302 - acc: 0.9950\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.8010e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.3173e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.7820e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.7558e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.9581e-04 - acc: 1.0000\n",
      "4 accuracy: 0.89\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4723 - acc: 0.7572\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0162 - acc: 0.9983\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.3414e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.3870e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.0790e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.1746e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.5247e-04 - acc: 1.0000\n",
      "5 accuracy: 0.88\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4843 - acc: 0.7650\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.2273e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.0106e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.6908e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.7751e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.2191e-04 - acc: 1.0000\n",
      "6 accuracy: 0.875\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4511 - acc: 0.7883\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0159 - acc: 0.9983\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.9714e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.6576e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.2613e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.2869e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.5995e-04 - acc: 1.0000\n",
      "7 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4599 - acc: 0.7767\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.0160 - acc: 0.9989\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.6482e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.2331e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.6882e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.3862e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.5138e-04 - acc: 1.0000\n",
      "8 accuracy: 0.88\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4799 - acc: 0.7467\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0211 - acc: 0.9961\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.1695e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.2525e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.9551e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.0838e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.4721e-04 - acc: 1.0000\n",
      "9 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4804 - acc: 0.7706\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0190 - acc: 0.9989\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.9361e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.0070e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.7117e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.8494e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.2554e-04 - acc: 1.0000\n",
      "10 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4657 - acc: 0.7700\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0159 - acc: 0.9989\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.8825e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.3594e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.7186e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.6428e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.9077e-04 - acc: 1.0000\n",
      "11 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4623 - acc: 0.7844\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0149 - acc: 0.9989\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.0700e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 4.5110e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.1094e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.1708e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.6092e-04 - acc: 1.0000\n",
      "12 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4658 - acc: 0.7656\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.6108e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.5199e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.1818e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.2552e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.6044e-04 - acc: 1.0000\n",
      "13 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4710 - acc: 0.7622\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0184 - acc: 0.9978\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.5754e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.3470e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.8566e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.8825e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.2299e-04 - acc: 1.0000\n",
      "14 accuracy: 0.9\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4779 - acc: 0.7644\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0172 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.5502e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.0333e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.5319e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.5033e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.7819e-04 - acc: 1.0000\n",
      "15 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4732 - acc: 0.7561\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0170 - acc: 0.9983\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.5540e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.3125e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.9769e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.0502e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.4426e-04 - acc: 1.0000\n",
      "16 accuracy: 0.89\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4465 - acc: 0.7878\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0141 - acc: 0.9983\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.9924e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.0714e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.8824e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.0717e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.4923e-04 - acc: 1.0000\n",
      "17 accuracy: 0.855\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4941 - acc: 0.7483\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0241 - acc: 0.9967\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.9398e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.5891e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.1895e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.1845e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.4887e-04 - acc: 1.0000\n",
      "18 accuracy: 0.88\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4742 - acc: 0.7639\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0161 - acc: 0.9983\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.9070e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.5649e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.0261e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.0035e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.3289e-04 - acc: 1.0000\n",
      "19 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.4858 - acc: 0.7633\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0206 - acc: 0.9978\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 9.1627e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.7088e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 5.1404e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 4.0131e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 3s - loss: 3.2189e-04 - acc: 1.0000\n",
      "20 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4715 - acc: 0.7672\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0158 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 9.2360e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.5475e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.9855e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.9293e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 3.1267e-04 - acc: 1.0000\n",
      "21 accuracy: 0.88\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4641 - acc: 0.7783\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0150 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 9.1566e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.5606e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.8754e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.7620e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.9977e-04 - acc: 1.0000\n",
      "22 accuracy: 0.88\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4787 - acc: 0.7622\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0157 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.5070e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.6954e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.0703e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.0667e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.3557e-04 - acc: 1.0000\n",
      "23 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4760 - acc: 0.7611\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0171 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.9500e-04 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      " - 2s - loss: 5.8441e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.3827e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.4279e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.7428e-04 - acc: 1.0000\n",
      "24 accuracy: 0.88\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4797 - acc: 0.7644\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0195 - acc: 0.9983\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.5916e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.9304e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.3639e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.3284e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.6314e-04 - acc: 1.0000\n",
      "25 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4817 - acc: 0.7489\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0218 - acc: 0.9983\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.3293e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.1175e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.7401e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.8934e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.3013e-04 - acc: 1.0000\n",
      "26 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4651 - acc: 0.7706\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0169 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.3070e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.0226e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.5835e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.6088e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.8955e-04 - acc: 1.0000\n",
      "27 accuracy: 0.875\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4802 - acc: 0.7628\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0173 - acc: 0.9989\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.8175e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 4.7574e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.5138e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.6489e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.1007e-04 - acc: 1.0000\n",
      "28 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4650 - acc: 0.7794\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0163 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 9.0435e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.5920e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.9233e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.8415e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 3.0668e-04 - acc: 1.0000\n",
      "29 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4840 - acc: 0.7617\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0169 - acc: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 9.7673e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 7.1697e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 5.4870e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 4.2245e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 3.3934e-04 - acc: 1.0000\n",
      "30 accuracy: 0.875\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6925 - acc: 0.6244\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6865 - acc: 0.6811\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6727 - acc: 0.8378\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6511 - acc: 0.8583\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6210 - acc: 0.9122\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5847 - acc: 0.9322\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5433 - acc: 0.9378\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.5013 - acc: 0.9450\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4583 - acc: 0.9539\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4178 - acc: 0.9611\n",
      "1 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6916 - acc: 0.6433\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6816 - acc: 0.7906\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6631 - acc: 0.8789\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6345 - acc: 0.9244\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5981 - acc: 0.9344\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5580 - acc: 0.9400\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5132 - acc: 0.9456\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4688 - acc: 0.9506\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4252 - acc: 0.9556\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3853 - acc: 0.9622\n",
      "2 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6920 - acc: 0.5689\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6842 - acc: 0.6911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6664 - acc: 0.8450\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6367 - acc: 0.9033\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5976 - acc: 0.9133\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5532 - acc: 0.9361\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5054 - acc: 0.9467\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4585 - acc: 0.9472\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4134 - acc: 0.9561\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3716 - acc: 0.9644\n",
      "3 accuracy: 0.855\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6914 - acc: 0.5506\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6815 - acc: 0.6722\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6620 - acc: 0.9061\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6314 - acc: 0.9128\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5931 - acc: 0.9233\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5494 - acc: 0.9322\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5021 - acc: 0.9506\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4564 - acc: 0.9583\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4117 - acc: 0.9606\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3706 - acc: 0.9633\n",
      "4 accuracy: 0.845\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6920 - acc: 0.5039\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6836 - acc: 0.8233\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6647 - acc: 0.8828\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6322 - acc: 0.8978\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5890 - acc: 0.9294\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5394 - acc: 0.9428\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.4917 - acc: 0.9378\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4394 - acc: 0.9578\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.3933 - acc: 0.9594\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3511 - acc: 0.9639\n",
      "5 accuracy: 0.88\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6923 - acc: 0.5889\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6857 - acc: 0.5917\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6707 - acc: 0.7528\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6447 - acc: 0.8733\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6097 - acc: 0.8833\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5679 - acc: 0.9267\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5223 - acc: 0.9494\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4764 - acc: 0.9539\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4315 - acc: 0.9622\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3896 - acc: 0.9650\n",
      "6 accuracy: 0.86\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6921 - acc: 0.5122\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6832 - acc: 0.7317\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6660 - acc: 0.6994\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6365 - acc: 0.9278\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5980 - acc: 0.9133\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5529 - acc: 0.9422\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5054 - acc: 0.9506\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4592 - acc: 0.9556\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4140 - acc: 0.9650\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3714 - acc: 0.9689\n",
      "7 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6914 - acc: 0.5639\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6814 - acc: 0.8267\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6631 - acc: 0.8200\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6334 - acc: 0.9106\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5950 - acc: 0.9289\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5493 - acc: 0.9144\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5035 - acc: 0.9444\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4551 - acc: 0.9550\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4105 - acc: 0.9572\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3676 - acc: 0.9617\n",
      "8 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6917 - acc: 0.5028\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6832 - acc: 0.7050\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6679 - acc: 0.8061\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6429 - acc: 0.8978\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6094 - acc: 0.9022\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5719 - acc: 0.9411\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5293 - acc: 0.9406\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4873 - acc: 0.9489\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4451 - acc: 0.9583\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4054 - acc: 0.9639\n",
      "9 accuracy: 0.855\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6920 - acc: 0.5189\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6845 - acc: 0.5794\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6713 - acc: 0.5972\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6508 - acc: 0.7233\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6251 - acc: 0.8028\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5956 - acc: 0.8622\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5627 - acc: 0.9011\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.5288 - acc: 0.9344\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4935 - acc: 0.9439\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4582 - acc: 0.9556\n",
      "10 accuracy: 0.855\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6915 - acc: 0.5039\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6813 - acc: 0.8461\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6637 - acc: 0.7944\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6372 - acc: 0.9100\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.6032 - acc: 0.9344\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5642 - acc: 0.9406\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5215 - acc: 0.9506\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4780 - acc: 0.9556\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4352 - acc: 0.9556\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3959 - acc: 0.9678\n",
      "11 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6919 - acc: 0.5667\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6834 - acc: 0.6211\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6649 - acc: 0.8367\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6360 - acc: 0.8844\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5989 - acc: 0.9233\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5567 - acc: 0.9400\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5116 - acc: 0.9483\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4664 - acc: 0.9544\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4232 - acc: 0.9567\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3823 - acc: 0.9628\n",
      "12 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6919 - acc: 0.6006\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6843 - acc: 0.8478\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6688 - acc: 0.8922\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6447 - acc: 0.9094\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6141 - acc: 0.9156\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5776 - acc: 0.9339\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5389 - acc: 0.9433\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4995 - acc: 0.9467\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4595 - acc: 0.9517\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4217 - acc: 0.9600\n",
      "13 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6918 - acc: 0.5644\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6832 - acc: 0.6828\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6673 - acc: 0.7172\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6425 - acc: 0.8100\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6098 - acc: 0.9167\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5714 - acc: 0.9333\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5309 - acc: 0.9394\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4883 - acc: 0.9572\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4471 - acc: 0.9628\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4074 - acc: 0.9694\n",
      "14 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6916 - acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6819 - acc: 0.5400\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6654 - acc: 0.6206\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6433 - acc: 0.6394\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6169 - acc: 0.7228\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5875 - acc: 0.7728\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5575 - acc: 0.8222\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.5265 - acc: 0.8667\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4958 - acc: 0.8922\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4662 - acc: 0.9117\n",
      "15 accuracy: 0.735\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6921 - acc: 0.5011\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6857 - acc: 0.5778\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6735 - acc: 0.5639\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6549 - acc: 0.7367\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6294 - acc: 0.8361\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5990 - acc: 0.8839\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5654 - acc: 0.9133\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.5293 - acc: 0.9406\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4918 - acc: 0.9511\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4571 - acc: 0.9567\n",
      "16 accuracy: 0.85\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6920 - acc: 0.5344\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6851 - acc: 0.6067\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6721 - acc: 0.8433\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6521 - acc: 0.8250\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6256 - acc: 0.9156\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5943 - acc: 0.9128\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5594 - acc: 0.9356\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.5212 - acc: 0.9450\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4843 - acc: 0.9589\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4462 - acc: 0.9606\n",
      "17 accuracy: 0.86\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6913 - acc: 0.6639\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6798 - acc: 0.8767\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6584 - acc: 0.8850\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6272 - acc: 0.9283\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5877 - acc: 0.9411\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5436 - acc: 0.9400\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.4967 - acc: 0.9489\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4508 - acc: 0.9472\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4066 - acc: 0.9628\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3657 - acc: 0.9683\n",
      "18 accuracy: 0.86\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6911 - acc: 0.6339\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6796 - acc: 0.7939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6585 - acc: 0.8217\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6273 - acc: 0.9039\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5891 - acc: 0.9300\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.5456 - acc: 0.9344\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5001 - acc: 0.9483\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4554 - acc: 0.9533\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4119 - acc: 0.9594\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.3713 - acc: 0.9656\n",
      "19 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6918 - acc: 0.5050\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6831 - acc: 0.6506\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6636 - acc: 0.8139\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6330 - acc: 0.8817\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5935 - acc: 0.9378\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5495 - acc: 0.9406\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5032 - acc: 0.9494\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4566 - acc: 0.9567\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4118 - acc: 0.9622\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3710 - acc: 0.9667\n",
      "20 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6914 - acc: 0.6094\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6805 - acc: 0.7822\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6596 - acc: 0.9100\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6278 - acc: 0.9161\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5863 - acc: 0.9317\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5392 - acc: 0.9444\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.4904 - acc: 0.9506\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4430 - acc: 0.9572\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.3974 - acc: 0.9589\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3559 - acc: 0.9678\n",
      "21 accuracy: 0.86\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6917 - acc: 0.5239\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6827 - acc: 0.6928\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6659 - acc: 0.7667\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6407 - acc: 0.8661\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6081 - acc: 0.9144\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5697 - acc: 0.9283\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5294 - acc: 0.9422\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4873 - acc: 0.9572\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4463 - acc: 0.9589\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4060 - acc: 0.9672\n",
      "22 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6915 - acc: 0.5444\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6818 - acc: 0.8506\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6637 - acc: 0.7294\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6384 - acc: 0.9106\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6031 - acc: 0.9161\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5637 - acc: 0.9422\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5215 - acc: 0.9406\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4782 - acc: 0.9567\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4364 - acc: 0.9583\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3968 - acc: 0.9628\n",
      "23 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6923 - acc: 0.5394\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6861 - acc: 0.6456\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6743 - acc: 0.7517\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6544 - acc: 0.9122\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6286 - acc: 0.9244\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5982 - acc: 0.9178\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5645 - acc: 0.9433\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.5290 - acc: 0.9439\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4924 - acc: 0.9494\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4566 - acc: 0.9533\n",
      "24 accuracy: 0.87\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6911 - acc: 0.5672\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6803 - acc: 0.7811\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6600 - acc: 0.9083\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6298 - acc: 0.9006\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5917 - acc: 0.9333\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5477 - acc: 0.9439\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5022 - acc: 0.9417\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4566 - acc: 0.9517\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4131 - acc: 0.9606\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3724 - acc: 0.9667\n",
      "25 accuracy: 0.885\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6918 - acc: 0.5272\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6841 - acc: 0.7222\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6685 - acc: 0.8478\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6420 - acc: 0.9072\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6061 - acc: 0.9200\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5637 - acc: 0.9317\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5184 - acc: 0.9467\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4732 - acc: 0.9550\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4292 - acc: 0.9578\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3874 - acc: 0.9617\n",
      "26 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6918 - acc: 0.5278\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6833 - acc: 0.7867\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6661 - acc: 0.8467\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6386 - acc: 0.8906\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6011 - acc: 0.9222\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5592 - acc: 0.9228\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5131 - acc: 0.9444\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4660 - acc: 0.9522\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4206 - acc: 0.9589\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3788 - acc: 0.9661\n",
      "27 accuracy: 0.875\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6919 - acc: 0.5383\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6839 - acc: 0.7967\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6667 - acc: 0.8844\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6379 - acc: 0.9211\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5995 - acc: 0.9278\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.5540 - acc: 0.9372\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5054 - acc: 0.9467\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4569 - acc: 0.9522\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4112 - acc: 0.9528\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3688 - acc: 0.9633\n",
      "28 accuracy: 0.865\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6918 - acc: 0.5794\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6834 - acc: 0.7028\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6667 - acc: 0.8772\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6409 - acc: 0.8594\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6060 - acc: 0.9322\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5666 - acc: 0.9422\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5257 - acc: 0.9311\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4816 - acc: 0.9522\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4398 - acc: 0.9567\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3984 - acc: 0.9611\n",
      "29 accuracy: 0.86\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6922 - acc: 0.4961\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6851 - acc: 0.5272\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6698 - acc: 0.6711\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6448 - acc: 0.8733\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6121 - acc: 0.9178\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5740 - acc: 0.9328\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5325 - acc: 0.9422\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4895 - acc: 0.9561\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4472 - acc: 0.9628\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4062 - acc: 0.9639\n",
      "30 accuracy: 0.87\n",
      "         binary      count      tfidf       freq\n",
      "count  30.00000  30.000000  30.000000  30.000000\n",
      "mean    0.92700   0.901333   0.878833   0.860833\n",
      "std     0.00857   0.005862   0.010059   0.025192\n",
      "min     0.91000   0.885000   0.855000   0.735000\n",
      "25%     0.92125   0.896250   0.870000   0.860000\n",
      "50%     0.92500   0.900000   0.880000   0.865000\n",
      "75%     0.93375   0.905000   0.885000   0.870000\n",
      "max     0.94500   0.910000   0.900000   0.885000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f83d944b320>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84d48acf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# run experiment\n",
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "    # prepare data for mode\n",
    "    Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "    # evaluate model on data for mode\n",
    "    results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "#plot results\n",
    "results.boxplot()\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciendo analisis de sentimiento para nuevos data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    # clean\n",
    "    tokens = clean_doc(review)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # convert to line\n",
    "    line = ' '.join(tokens)\n",
    "    # encode\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para ejemplos de prueba para probar el modelo, anterior mente realizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE (100.000%)\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (100.000%)\n"
     ]
    }
   ],
   "source": [
    "# test positive text\n",
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aqui voy a colocar todo el codigo unido, porque me esta saliendo todo 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_253 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_254 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 9s - loss: 0.4394 - acc: 0.8070\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0468 - acc: 0.9950\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.3108e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.1588e-04 - acc: 1.0000\n",
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE (55.229%)\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (65.962%)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "# split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    # clean\n",
    "    tokens = clean_doc(review)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # convert to line\n",
    "    line = ' '.join(tokens)\n",
    "    # encode\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab)\n",
    "test_docs, ytest = load_clean_dataset(vocab)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
    "# define network\n",
    "n_words = Xtrain.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)\n",
    "# test positive text\n",
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
